<!DOCTYPE html>
<html><title>raspberry pi 3B&#43; hungs with mmc_rescan</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="bLmaFyyXugiCqSup-eIIIx0B4CngtdF_svyMMKQbS5E" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=10, minimum-scale=0.5, user-scalable=yes">
<meta name="apple-mobile-web-app-capable" content="yes" />
<script src="https://hypothes.is/embed.js" async></script>


<link rel="stylesheet" href="/braindump/css/main.min.56b43bcbc759a196f9dd525bee62bdec45a29ba95902bbab74a4f7ad2ea3e8cb.css"/>

<link rel="stylesheet" href="/braindump/css/links.min.4bf1990b213fb76d2a66153dc6ef9a57d49f536c6baf6e7f19a7948470a38141.css"/>

<link rel="stylesheet" href="/braindump/css/konubinix.min.cf2d0f36945178c647979ee68f1830892e06aa3d481d8913728b5c7462b97f6e.css"/>

<link rel="stylesheet" href="/ipfs/QmQFVQS89fv1XUNFcjwCKDePzckuT9kpuAVNYUwNdEfYcv/css/all.css"/>
<link rel="shortcut icon" href="/ipfs/QmVFwYV7YRZLKU3ybN1hW4jqfyZGPLKJwd8toN2wHz5UAD?a.png" type="image/x-icon" />

<body><header>
  <div>
	<a href="/braindump//"><h5 class="site-title">Konubinix&#39; opinionated web of thoughts</h5></a>
  </div>
  <span>
	<a href="/blog/"><i class="icon fas fa-blog"></i></a>
	<a href="/braindump/posts/"><i class="icon fas fa-brain"></i></a>
    <a href="mailto:konubinixweb@gmail.com"><i class="icon fas fa-envelope-square"></i></a>
    <a href="https://github.com/konubinix"><i class="icon fab fa-github-square"></i></a>
    <a href="https://linkedin.com/in/samuel-loury-61259040"><i class="icon fab fa-linkedin"></i></a>
	<a href="/braindump/graph.html"><i class="icon fas fa-project-diagram"></i></a>
	<a href="/braindump/index.xml"><i class="icon fas fa-rss-square"></i></a>
	<a href="/braindump/tags/"><i class="icon fa fa-tag"></i></a>
	<a href="/braindump/braindump_search"><i class="icon fa fa-search"></i></a>
  </span>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
		<p class="lead">
        <h1>Raspberry Pi 3B&#43; Hungs With Mmc_rescan</h1>
		<span class="badge badge-pill badge-warning"><a href="/braindump/tags/fleeting/">Fleeting</a></span></p>
        <p><a href="/braindump/posts/raspberry_pi/">raspberry pi</a></p>
<p>After moving from armv7 to arm64, I get from time to time those errors.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023] INFO: task kworker/2:0:725409 blocked for more than 362 seconds.
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]       Tainted: G         C         6.1.21-v8+ #1642
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023] &#34;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&#34; disables this message.
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023] task:kworker/2:0     state:D stack:0     pid:725409 ppid:2      flags:0x00000008
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023] Workqueue: events_freezable mmc_rescan
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023] Call trace:
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  __switch_to+0xf8/0x1e0
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  __schedule+0x2a8/0x830
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  schedule+0x60/0x100
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  __mmc_claim_host+0xbc/0x208
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  mmc_get_card+0x3c/0x50
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  mmc_sd_detect+0x28/0x98
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  mmc_rescan+0xa0/0x2c8
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  process_one_work+0x208/0x480
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  worker_thread+0x50/0x428
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  kthread+0xfc/0x110
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:52:11 2023]  ret_from_fork+0x10/0x20
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023] INFO: task kworker/2:0:725409 blocked for more than 120 seconds.
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]       Tainted: G         C         6.1.21-v8+ #1642
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023] &#34;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&#34; disables this message.
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023] task:kworker/2:0     state:D stack:0     pid:725409 ppid:2      flags:0x00000008
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023] Workqueue: events_freezable mmc_rescan
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023] Call trace:
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  __switch_to+0xf8/0x1e0
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  __schedule+0x2a8/0x830
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  schedule+0x60/0x100
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  __mmc_claim_host+0xbc/0x208
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  mmc_get_card+0x3c/0x50
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  mmc_sd_detect+0x28/0x98
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  mmc_rescan+0xa0/0x2c8
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  process_one_work+0x208/0x480
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  worker_thread+0x50/0x428
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  kthread+0xfc/0x110
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:56:12 2023]  ret_from_fork+0x10/0x20
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023] INFO: task kworker/2:0:725409 blocked for more than 241 seconds.
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]       Tainted: G         C         6.1.21-v8+ #1642
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023] &#34;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&#34; disables this message.
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023] task:kworker/2:0     state:D stack:0     pid:725409 ppid:2      flags:0x00000008
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023] Workqueue: events_freezable mmc_rescan
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023] Call trace:
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  __switch_to+0xf8/0x1e0
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  __schedule+0x2a8/0x830
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  schedule+0x60/0x100
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  __mmc_claim_host+0xbc/0x208
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  mmc_get_card+0x3c/0x50
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  mmc_sd_detect+0x28/0x98
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  mmc_rescan+0xa0/0x2c8
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  process_one_work+0x208/0x480
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  worker_thread+0x50/0x428
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  kthread+0xfc/0x110
</span></span><span style="display:flex;"><span>[Tue Jun 27 17:58:13 2023]  ret_from_fork+0x10/0x20
</span></span></code></pre></div><p>I disabled the swap, because I don&rsquo;t want my sd card to be overused and I&rsquo;d
rather have some programs stopping rather than my machine becoming
unresponsive. Because it is a node in <a href="/braindump/posts/nomad/">nomad</a>, I trust nomad for already
stopping programs that consume too much ram.</p>
<p>I tried using <a href="/braindump/posts/fight_flash_fraud/">f3</a> to scan the card and no issues were detected.</p>
<p>Also, it appears to be a well known issue.</p>
<ul>
<li><a href="https://github.com/raspberrypi/linux/issues/2810">https://github.com/raspberrypi/linux/issues/2810</a></li>
<li><a href="https://github.com/agherzan/meta-raspberrypi/issues/1088">https://github.com/agherzan/meta-raspberrypi/issues/1088</a></li>
</ul>
<p>It seems to appear when the card needs to flush the <a href="/braindump/posts/dirty_pages/">dirty pages</a> on
disk. The disk is so slow compared to the RAM being filled up that linux makes a
pause.</p>
<p>It can be due to</p>
<ol>
<li>a card too slow,</li>
<li>a card starting to fail,</li>
<li>some issues with the card reader,</li>
</ol>
<p>Some people put the card in a SD-USB adapter and boot over USB (<a href="/braindump/posts/pi_3_b_booting_on_the_usb_card/">Pi 3 B+ booting
on the usb card</a>). This increased the speed, hence we cannot tell whether it
fixes the slowness issue or some hardware issue. At least, it can tell that the
sd card is not at fault.</p>
<p>Some also boot over a SSD drive.</p>
<p>Some decrease the values of <a href="/braindump/posts/vm_dirty_ratio_and_vm_dirty_background_ratio/">vm.dirty_ratio and vm.dirty_background_ratio</a> to make
those flush happen more often, leading to decreasing overall performances but
also the odds of getting into this situation. see <a href="/braindump/posts/linux_kernel_panic_issue_how_to_fix_hung_task_timeout_secs_and_blocked_for_more_than_120_seconds_problem/">Linux Kernel panic issue: How to fix hung_task_timeout_secs and blocked for more than 120 seconds problem</a></p>
<h2 id="trying-to-reproduce-the-issue"><span class="timestamp-wrapper"><span class="timestamp">[2023-06-28 Wed] </span></span> Trying to reproduce the issue</h2>
<p>This should happen when a big file is being read (creating many <a href="/braindump/posts/dirty_pages/">dirty pages</a>)
while a process is asking for much memory.</p>
<p>EDIT <span class="timestamp-wrapper"><span class="timestamp">[2023-08-03 Thu] </span></span> I did not continue this</p>
<h2 id="setting-the-dirty-ratio-values"><span class="timestamp-wrapper"><span class="timestamp">[2023-07-06 Thu] </span></span> Setting the dirty_ratio values</h2>
<p>The current value on my rpi is</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ssh burberry sudo sysctl -a |grep dirty_|grep ratio
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>vm.dirty_background_ratio = 10
</span></span><span style="display:flex;"><span>vm.dirty_ratio = 20
</span></span></code></pre></div><p>The suggested values are</p>
<blockquote>
<p>sudo sysctl -w vm.dirty_ratio=10
sudo sysctl -w vm.dirty_background_ratio=5
sudo sysctl -p</p>
<p>&mdash; <a href="https://github.com/raspberrypi/linux/issues/2810">https://github.com/raspberrypi/linux/issues/2810</a></p>
</blockquote>
<!--quoteend-->
<blockquote>
<p>vm.dirty_background_ratio = 5
vm.dirty_ratio = 10</p>
<p>&mdash; <a href="https://www.blackmoreops.com/2014/09/22/linux-kernel-panic-issue-fix-hung_task_timeout_secs-blocked-120-seconds-problem/">https://www.blackmoreops.com/2014/09/22/linux-kernel-panic-issue-fix-hung_task_timeout_secs-blocked-120-seconds-problem/</a></p>
</blockquote>
<p>It did not change the issue.</p>
<h2 id="adding-reserved-memory-in-nomad--nomad-dot-md"><span class="timestamp-wrapper"><span class="timestamp">[2023-07-06 Thu] </span></span> Adding <a href="https://developer.hashicorp.com/nomad/docs/configuration/client#reserved">reserved memory</a> in <a href="/braindump/posts/nomad/">nomad</a></h2>
<p>Those machines are actually nomad agents.
I added this block in their configuration.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-hcl" data-lang="hcl"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">reserved</span> {
</span></span><span style="display:flex;"><span>  &#34;memory&#34; <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">100</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>So far, so good. I did not have any issue for about a whole week.</p>
<p><span class="timestamp-wrapper"><span class="timestamp">[2023-07-12 Wed] </span></span> I got a few of them today, moving on to using RPI4B instead.</p>
<h2 id="trying-soft-reboot-from-time-to-time"><span class="timestamp-wrapper"><span class="timestamp">[2023-08-03 Thu] </span></span> Trying soft reboot from time to time</h2>
<blockquote>
<p>I have solved the issue for me, but it is not a general solutions. Anyway i describe here my workarround maybe it helps some other to solve the problem.
As is wrote earlier, my setup is a raspberry pi kubernetes cluster with 3 worker and 1 master. The master and 2 of the worker are Raspberry Pi 3 B+ and one worker is a older Pi 3 B. The problem happens on every worker after 24-28 hours when the worker runs my home automation software called FHEM. When the problem happens, kubernetes rescheduled FHEM to run on another worker, so every worker has the problem. FHEM collects many data from external sensors like temperature in rooms or values from the heating or wind data. This many data is written via GlusterFS to a GlusterFS cluster which is based on two Raspberry 1 with USB Sticks for the data. Kubernetes and Docker wrote their status messages and other stuff to the local SD card. My solution is to restart FHEM every night at 00:05:00 am. After that the problem never happens again, it is running stable since 5. june. The restart command shuts down the kubernetes pod and kubernetes restarts the pod after a few seconds. This means that in the background the docker container in which the FHEM runs will be shutdown and restarted.</p>
<p>&mdash; <a href="https://github.com/raspberrypi/linux/issues/1885">https://github.com/raspberrypi/linux/issues/1885</a></p>
</blockquote>
<!--quoteend-->
<blockquote>
<figure><img src="https://imgs.xkcd.com/comics/hard_reboot.png">
</figure>

<p>&mdash; <a href="https://xkcd.com/1495/">https://xkcd.com/1495/</a></p>
</blockquote>
<!--more-->
<h2 id="notes-linking-here">Notes linking here</h2>
<ul>
<li><a href="/braindump/posts/pi_3_b_booting_on_the_usb_card/">Pi 3 B+ booting on the usb card</a></li>
</ul>
<h2 id="permalink"><a href="https://konubinix.eu/braindump/32918d92-bbb0-4abe-b1c3-d79bddbdb198?title=raspberry_pi_3b_hungs_with_mmc_rescan">Permalink</a></h2>
      </div>
	  <aside class="date"><time>Last updated: 03 Aug 2023</time></aside>
	  <aside class="date"><time>Published&nbsp;&nbsp;&nbsp;: 28 Jun 2023</time></aside>
    </div>
  </div>
</div>

<script src="/braindump/js/URI.js" type="text/javascript"></script>

<script src="/braindump/js/page.js" type="text/javascript"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
